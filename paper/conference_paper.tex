\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{High-Resolution Patch-Based Semi-Supervised Learning for Pancreatic Cancer Segmentation\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Kursat [Supervisor Name]}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{University of Vilnius}\\
Vilnius, Lithuania \\
email@address.com}
}

\maketitle

\begin{abstract}
Accurate segmentation of the pancreas and pancreatic tumors from CT scans is critical for diagnosis and treatment planning but is hindered by the organ's high anatomical variability and the scarcity of pixel-level annotations. Standard deep learning approaches (e.g., 3D U-Net, V-Net) often necessitate downsampling volumes to fit GPU memory constraints, resulting in a critical loss of spatial resolution and feature detail. In this work, we propose a two-stage optimization strategy. First, we address the resolution bottleneck by implementing a High-Resolution Patch-Based U-Net, which processes 256x256 crops from original 512x512 slices. This yields a significant performance leap from 0.73 to 0.85 Dice. Second, we investigate the data efficiency of this framework using Semi-Supervised Learning (SSL). We adapt FixMatch with consistent augmentation to the patch domain. Our extensive experiments demonstrate that while SSL improves generalization with 50\% labeled data, Mean Teacher (0.83 Dice) significantly outperforms FixMatch (0.69 Dice), approaching fully supervised performance within a 2\% margin. Our method achieves state-of-the-art comparable performance while offering a tunable trade-off between annotation cost and accuracy.
\end{abstract}

\begin{IEEEkeywords}
Pancreas Segmentation, Semi-Supervised Learning, Patch-Based Deep Learning, Mean Teacher, CT Imaging
\end{IEEEkeywords}

\section{Introduction}
Pancreatic cancer remains a devastating disease with a 5-year survival rate of merely 9\% \cite{Cancerstatistics2023}. Early detection via Computed Tomography (CT) is critical but hindered by the organ's irregular shape and low contrast. Standard deep learning approaches (e.g., U-Net \cite{unet}, V-Net \cite{vnet}) often downsample large CT volumes to fit GPU memory, sacrificing the resolution needed to detect fine tumor boundaries.

In this work, we propose a \textbf{High-Resolution Patch-Based Framework} that solves this bottleneck. We show that maintaining native resolution is more critical than global 3D context, boosting Dice scores from a baseline of 0.70 to 0.85. Furthermore, we optimize this framework using Semi-Supervised Learning (SSL) algorithms—FixMatch and Mean Teacher—to reduce annotation costs. Our rigorous experiments reveal that Mean Teacher provides superior stability for volumetric segmentation.

\section{Related Work}

\subsection{Medical Image Segmentation}
Fully Convolutional Networks (FCNs) revolutionized medical imaging. U-Net \cite{unet}, with its encoder-decoder structure and skip connections, remains the gold standard. For 3D volumetric data, V-Net \cite{vnet} and 3D U-Net extended this architecture using 3D convolutions. However, these 3D models suffer from high memory consumption, often necessitating input downsampling (e.g., $512^3 \rightarrow 128^3$), which causes significant information loss for small anatomical structures like the pancreatic duct.

\subsection{Semi-Supervised Learning (SSL)}
SSL aims to leverage unlabeled data. Two dominant paradigms have emerged:
\begin{itemize}
    \item \textbf{Consistency Regularization:} Methods like Mean Teacher \cite{tarvainen2017mean} enforce that perturbations of the input should yield consistent predictions.
    \item \textbf{Pseudo-Labeling:} Methods like FixMatch \cite{fixmatch} use the model's high-confidence predictions on weakly augmented data as labels for strongly augmented versions.
\end{itemize}
While successful in natural images (CIFAR-10), their application to high-resolution medical patches remains under-explored.

\section{Methodology}

\subsection{Primary Contribution: High-Resolution Patching}
The core novelty of this work is the shift from "Complex Architectures on Low-Res Inputs" to "Standard Architectures on High-Res Inputs."
Standard pipelines resize $512 \times 512$ slices to $256 \times 256$, discarding 75\% of the pixels to fit GPU memory. We show that this \textbf{resolution loss} is the primary bottleneck, capping performance at $\approx 0.73$ Dice.
By training on $256 \times 256$ crops (patches) from the original images, we preserve 100\% of the fine anatomical details. This simple change boosts performance to \textbf{0.85 Dice}, surpassing SOTA methods that rely on complex attention mechanisms or transformers.

\subsection{Secondary Analysis: Semi-Supervised Efficiency}
To further optimize this high-res framework, we explore SSL (FixMatch, Mean Teacher) to reduce the annotation cost.

\subsubsection{FixMatch (Threshold-Based)}
For an unlabeled patch $u_b$, we apply weak augmentation $\alpha(\cdot)$ and strong augmentation $\mathcal{A}(\cdot)$. The loss is:
\begin{equation}
    \mathcal{L}_u = \mathbb{I}(\max(p_m(y|\alpha(u_b))) \ge \tau) H(\hat{p}, p_m(y|\mathcal{A}(u_b)))
\end{equation}
where $\tau=0.95$ is the confidence threshold and $H$ is cross-entropy.

\subsubsection{Mean Teacher (EMA-Based)}
We maintain a Student ($f_\theta$) and a Teacher ($f_{\theta'}$). The teacher weights are updated via Exponential Moving Average (EMA):
\begin{equation}
    \theta'_t = \alpha \theta'_{t-1} + (1-\alpha)\theta_t
\end{equation}
where $\alpha=0.999$. The consistency loss is the Mean Squared Error (MSE) between their predictions under different noise conditions.

\section{Experiments}

\subsection{Implementation Details}
\begin{table}[htbp]
\caption{Hyperparameter Configuration}
\begin{center}
\begin{tabular}{lc}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
Patch Size & $256 \times 256$ \\
Batch Size & 8 (4 Labeled + 4 Unlabeled) \\
Optimizer & Adam ($lr=1e-4$) \\
Training Epochs & 100 \\
FixMatch Threshold ($\tau$) & 0.95 \\
Mean Teacher EMA ($\alpha$) & 0.999 \\
\hline
\end{tabular}
\end{center}
\label{tab:params}
\end{table}

\subsection{Phase I: Resolution Impact}
Comparing standard resizing vs. our patch approach:
\begin{itemize}
    \item \textbf{Resized (256x256):} 0.73 Dice.
    \item \textbf{Patch (High-Res):} \textbf{0.85 Dice}.
\end{itemize}
This 12\% improvement confirms that resolution is the primary bottleneck.

\subsection{Phase II: SSL Data Efficiency}
\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{thesis_plot_efficiency.png}}
\caption{Data Efficiency: 50\% labeled data recovers $\approx 80\%$ of performance.}
\label{fig:efficiency}
\end{figure}

\subsection{Phase III: The Generalization Gap}
Figure \ref{fig:gap} illustrates a critical finding. High patch-level accuracy does not guarantee 3D segmentation success.
\begin{itemize}
    \item \textbf{Patch Metrics:} High IoU (0.53) even with 10\% labels.
    \item \textbf{3D Metrics:} Catastrophic failure (0.45 Dice) without sufficient global context anchoring.
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.45\textwidth]{thesis_plot_gap.png}}
\caption{The Generalization Gap: Patch metrics (Orange) vs 3D Dice (Blue).}
\label{fig:gap}
\end{figure}

\section{State of the Art Comparison}
A crucial question is where our results stand compared to recent literature on the NIH Pancreas-CT dataset. As shown in Table \ref{tab:sota}, most competing methods achieve Dice scores in the range of 0.79-0.81.

\begin{table}[htbp]
\caption{Comparison with SOTA Methods (NIH Dataset)}
\begin{center}
\begin{tabular}{lcc}
\hline
\textbf{Method} & \textbf{Data \%} & \textbf{Dice Score} \\
\hline
V-Net (Baseline) \cite{vnet} & 100\% & $0.73 \pm 0.10$ \\
Attention U-Net \cite{attunet} & 100\% & $0.80 \pm 0.02$ \\
\textbf{Ours (Supervised)} & \textbf{100\%} & $\mathbf{0.849 \pm 0.04}$ \\
\hline
FixMatch (SSL) & 50\% & 0.6870 \\
\textbf{Mean Teacher (SSL)} & \textbf{50\%} & \textbf{0.8292} \\
\hline
\end{tabular}
\end{center}
\label{tab:sota}
\end{table}

Our Mean Teacher approach (0.83 Dice) significantly outperforms FixMatch and matches the efficiency of fully supervised baselines using only half the data.

\section{Qualitative Analysis}
\begin{figure}[htbp]
\centerline{\includegraphics[width=0.48\textwidth]{vis_combined.png}}
\caption{Qualitative Results (Cases 001, 004, 006). Rows show different patient scans. Left: CT. Middle: Ground Truth (Green). Right: Prediction (Red). The model consistently achieves high overlap (Dice > 0.85) across varying anatomies.}
\label{fig:vis}
\end{figure}

\section{Discussion}
We observed that Semi-Supervised Learning (SSL) can nearly match full supervision.
\begin{itemize}
    \item \textbf{Mean Teacher Success:} Achieving 0.83 Dice with 50\% data proves that consistency regularization is effective for soft organ boundaries.
    \item \textbf{FixMatch Limitation:} FixMatch stalled at 0.69 Dice, likely because hard thresholding ($\tau>0.95$) is too aggressive for ambiguous medical images.
    \item \textbf{Conclusion:} High-Resolution inputs combined with Mean Teacher provide the optimal balance between accuracy and annotation effort.
\end{itemize}

\section{Conclusion}
We present a High-Resolution Patch-Based framework achieving SOTA results (0.85 Dice) on the NIH dataset. Our findings challenge the trend of increasingly complex architectures, showing that resolution preservation is the key bottleneck. We confirm that SSL (specifically Mean Teacher, 0.83 Dice) is highly efficient, recovering 98\% of supervised performance with only half the labels.

\begin{thebibliography}{00}
\bibitem{Cancerstatistics2023} R. L. Siegel et al., "Cancer statistics, 2023," CA: A Cancer Journal for Clinicians.
\bibitem{unet} O. Ronneberger et al., "U-net: Convolutional networks for biomedical image segmentation," MICCAI 2015.
\bibitem{vnet} F. Milletari et al., "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation," 3DV 2016.
\bibitem{fixmatch} K. Sohn et al., "FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence," NeurIPS 2020.
\bibitem{tarvainen2017mean} A. Tarvainen and H. Valpola, "Mean teachers are better role models," NeurIPS 2017.
\bibitem{attunet} O. Oktay et al., "Attention U-Net: Learning Where to Look for the Pancreas," 2018.
\end{thebibliography}

\end{document}
